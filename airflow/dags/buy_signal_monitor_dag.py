"""
Buy Signal Monitoring DAG
ìŠ¤í¬ë¦¬ë‹ ì „ëµ ìë™ ëª¨ë‹ˆí„°ë§ ë° ë§¤ìˆ˜ ì‹œê·¸ë„ ìƒì„±

ìŠ¤ì¼€ì¤„: */5 * * * * (5ë¶„ë§ˆë‹¤)
ëª©ì : HappyStockLife ë°±í…ŒìŠ¤íŒ… í”Œë«í¼ì— ë“±ë¡ëœ ê´€ì‹¬ ì „ëµì„ ìë™ìœ¼ë¡œ ì²´í¬í•˜ê³ ,
      ì¡°ê±´ì— ë§ëŠ” ì¢…ëª©ì´ ë°œê²¬ë˜ë©´ ë§¤ìˆ˜ ì‹œê·¸ë„ì„ ìƒì„±í•˜ì—¬ ì•Œë¦¼ì„ ì „ì†¡í•©ë‹ˆë‹¤.

ì‘ì„±ì¼: 2025-01-18
"""

from datetime import datetime, timedelta, date
from airflow import DAG
from airflow.operators.python import PythonOperator
import logging
import pandas as pd
import json
from sqlalchemy import text

import sys
import os
sys.path.insert(0, os.path.dirname(__file__))

from common_functions import (
    get_db_engine,
    get_supabase_connection,
    is_trading_day
)

logger = logging.getLogger(__name__)

# ========================================
# DAG ê¸°ë³¸ ì„¤ì •
# ========================================

default_args = {
    'owner': 'stock-system',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 18),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=2),
}

dag = DAG(
    dag_id='buy_signal_monitor_dag',
    default_args=default_args,
    description='ë§¤ìˆ˜ ì‹œê·¸ë„ ìë™ ëª¨ë‹ˆí„°ë§ (5ë¶„ë§ˆë‹¤)',
    schedule_interval='*/5 * * * *',  # 5ë¶„ë§ˆë‹¤ ì‹¤í–‰
    catchup=False,
    max_active_runs=1,
    tags=['buy-signal', 'monitoring', 'screening', 'happystocklife']
)

# ========================================
# Task 1: í™œì„±í™”ëœ ì „ëµ ì¡°íšŒ
# ========================================

def task_fetch_active_strategies(**context):
    """
    Supabaseì—ì„œ í™œì„±í™”ëœ ê´€ì‹¬ ì „ëµì„ ì¡°íšŒí•©ë‹ˆë‹¤.

    Returns:
        í™œì„± ì „ëµ ìˆ˜
    """
    logger.info("="*80)
    logger.info("Task 1: í™œì„±í™”ëœ ì „ëµ ì¡°íšŒ ì‹œì‘")
    logger.info("="*80)

    try:
        supabase_engine = get_supabase_connection()

        query = """
            SELECT
                id,
                user_id,
                name,
                description,
                conditions,
                target_market,
                notification_enabled,
                last_run_at
            FROM saved_strategies
            WHERE is_active = true
            ORDER BY created_at DESC
        """

        strategies = pd.read_sql(query, supabase_engine)

        logger.info(f"í™œì„±í™”ëœ ì „ëµ: {len(strategies)}ê°œ")

        if strategies.empty:
            logger.warning("í™œì„±í™”ëœ ì „ëµì´ ì—†ìŠµë‹ˆë‹¤.")
            return 0

        # ì „ëµ ëª©ë¡ ë¡œê¹…
        for _, strategy in strategies.iterrows():
            logger.info(f"  - {strategy['name']} (ID: {strategy['id']})")

        # XComìœ¼ë¡œ ë‹¤ìŒ Taskì— ì „ë‹¬
        context['task_instance'].xcom_push(
            key='active_strategies',
            value=strategies.to_json(orient='records', date_format='iso')
        )

        logger.info("í™œì„± ì „ëµ ì¡°íšŒ ì™„ë£Œ")
        return len(strategies)

    except Exception as e:
        logger.error(f"í™œì„± ì „ëµ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise


# ========================================
# Task 2: ìŠ¤í¬ë¦¬ë‹ ì‹¤í–‰
# ========================================

def task_run_screening(**context):
    """
    ê° ì „ëµì˜ ì¡°ê±´ìœ¼ë¡œ ìŠ¤í¬ë¦¬ë‹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

    Returns:
        ë°œê²¬ëœ ì‹œê·¸ë„ ìˆ˜
    """
    logger.info("="*80)
    logger.info("Task 2: ìŠ¤í¬ë¦¬ë‹ ì‹¤í–‰ ì‹œì‘")
    logger.info("="*80)

    # ì „ëµ ì¡°íšŒ
    strategies_json = context['task_instance'].xcom_pull(
        task_ids='fetch_active_strategies',
        key='active_strategies'
    )

    if not strategies_json:
        logger.warning("ì „ëµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0

    strategies = pd.read_json(strategies_json, orient='records')

    # ì˜¤ëŠ˜ ë‚ ì§œ í™•ì¸
    today = date.today()

    # ì£¼ë§ì´ë©´ ê°€ì¥ ìµœê·¼ ê±°ë˜ì¼ ì‚¬ìš©
    if not is_trading_day(today):
        logger.warning(f"{today}ëŠ” ê±°ë˜ì¼ì´ ì•„ë‹™ë‹ˆë‹¤. ìŠ¤í¬ë¦¬ë‹ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return 0

    # ì£¼ì‹ ë°ì´í„° ì¡°íšŒ (stock_prices + technical_indicators)
    main_db_engine = get_db_engine()

    query = """
        SELECT
            sp.ticker,
            s.company_name as name,
            sp.close_price as price,
            sp.volume,
            ((sp.close_price - LAG(sp.close_price, 1) OVER (PARTITION BY sp.ticker ORDER BY sp.date))
             / LAG(sp.close_price, 1) OVER (PARTITION BY sp.ticker ORDER BY sp.date) * 100) as change_percent,
            ti.rsi,
            ti.macd,
            ti.macd_signal,
            ti.ma_20,
            ti.ma_50,
            ti.ma_200,
            ti.bollinger_upper,
            ti.bollinger_lower
        FROM stock_prices sp
        JOIN stocks s ON sp.ticker = s.ticker
        LEFT JOIN technical_indicators ti ON sp.ticker = ti.ticker AND sp.date = ti.date
        WHERE sp.date = :today
        AND s.is_active = true
    """

    stock_data = pd.read_sql(query, main_db_engine, params={'today': today})

    logger.info(f"ì˜¤ëŠ˜({today}) ì£¼ì‹ ë°ì´í„°: {len(stock_data)}ê°œ")

    if stock_data.empty:
        logger.warning("ì˜¤ëŠ˜ ì£¼ì‹ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0

    # ì „ëµë³„ ìŠ¤í¬ë¦¬ë‹ ì‹¤í–‰
    all_signals = []

    for _, strategy in strategies.iterrows():
        strategy_name = strategy['name']
        conditions = strategy['conditions']  # JSONB (list of dict)

        logger.info(f"ì „ëµ '{strategy_name}' ìŠ¤í¬ë¦¬ë‹ ì‹œì‘...")

        try:
            # ì¡°ê±´ ì ìš©
            matched_stocks = apply_screening_conditions(stock_data.copy(), conditions)

            if not matched_stocks.empty:
                logger.info(f"  âœ… {len(matched_stocks)}ê°œ ì¢…ëª© ë°œê²¬")

                for _, stock in matched_stocks.iterrows():
                    signal = {
                        'strategy_id': strategy['id'],
                        'user_id': strategy['user_id'],
                        'symbol': stock['ticker'],
                        'name': stock['name'],
                        'price': float(stock['price']) if pd.notna(stock['price']) else None,
                        'change_percent': float(stock['change_percent']) if pd.notna(stock['change_percent']) else None,
                        'volume': int(stock['volume']) if pd.notna(stock['volume']) else None,
                        'indicators': {
                            'rsi': float(stock['rsi']) if pd.notna(stock['rsi']) else None,
                            'macd': float(stock['macd']) if pd.notna(stock['macd']) else None,
                            'macd_signal': float(stock['macd_signal']) if pd.notna(stock['macd_signal']) else None,
                            'ma_20': float(stock['ma_20']) if pd.notna(stock['ma_20']) else None,
                            'ma_50': float(stock['ma_50']) if pd.notna(stock['ma_50']) else None
                        },
                        'matched_conditions': describe_matched_conditions(stock, conditions)
                    }
                    all_signals.append(signal)
            else:
                logger.info(f"  â„¹ï¸  ì¡°ê±´ ë§Œì¡± ì¢…ëª© ì—†ìŒ")

        except Exception as e:
            logger.error(f"  âŒ ì „ëµ '{strategy_name}' ìŠ¤í¬ë¦¬ë‹ ì‹¤íŒ¨: {str(e)}")

    logger.info(f"ì´ {len(all_signals)}ê°œ ì‹œê·¸ë„ ë°œê²¬")

    # XComìœ¼ë¡œ ì „ë‹¬
    context['task_instance'].xcom_push(key='signals', value=all_signals)

    return len(all_signals)


def apply_screening_conditions(stock_data: pd.DataFrame, conditions: list) -> pd.DataFrame:
    """
    ìŠ¤í¬ë¦¬ë‹ ì¡°ê±´ì„ ì ìš©í•˜ì—¬ ì¢…ëª©ì„ í•„í„°ë§í•©ë‹ˆë‹¤.

    Args:
        stock_data: ì£¼ì‹ ë°ì´í„° DataFrame
        conditions: ì¡°ê±´ ë¦¬ìŠ¤íŠ¸ (JSON í˜•íƒœ)

    Returns:
        í•„í„°ë§ëœ DataFrame
    """
    filtered = stock_data.copy()

    for condition in conditions:
        field = condition.get('field')
        operator = condition.get('operator')
        value = condition.get('value')

        if not field or not operator:
            continue

        # í•„ë“œê°€ ë°ì´í„°ì— ì—†ìœ¼ë©´ ìŠ¤í‚µ
        if field not in filtered.columns:
            logger.warning(f"í•„ë“œ '{field}'ê°€ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
            continue

        try:
            if operator == '>':
                filtered = filtered[filtered[field] > value]
            elif operator == '<':
                filtered = filtered[filtered[field] < value]
            elif operator == '>=':
                filtered = filtered[filtered[field] >= value]
            elif operator == '<=':
                filtered = filtered[filtered[field] <= value]
            elif operator == '==':
                filtered = filtered[filtered[field] == value]
            elif operator == 'between':
                if isinstance(value, list) and len(value) == 2:
                    filtered = filtered[(filtered[field] >= value[0]) & (filtered[field] <= value[1])]
            else:
                logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ì—°ì‚°ì: {operator}")

        except Exception as e:
            logger.error(f"ì¡°ê±´ ì ìš© ì¤‘ ì˜¤ë¥˜: {condition}, ì˜¤ë¥˜: {str(e)}")

    return filtered


def describe_matched_conditions(stock: pd.Series, conditions: list) -> list:
    """
    ë§¤ì¹­ëœ ì¡°ê±´ì„ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.

    Args:
        stock: ì¢…ëª© ë°ì´í„°
        conditions: ì¡°ê±´ ë¦¬ìŠ¤íŠ¸

    Returns:
        ì¡°ê±´ ì„¤ëª… ë¦¬ìŠ¤íŠ¸
    """
    descriptions = []

    for condition in conditions:
        field = condition.get('field')
        operator = condition.get('operator')
        value = condition.get('value')

        if not field or not operator:
            continue

        # í•„ë“œ ê°’
        field_value = stock.get(field)
        if pd.isna(field_value):
            continue

        # ì„¤ëª… ìƒì„±
        if operator == '>':
            descriptions.append(f"{field} > {value} (í˜„ì¬: {field_value:.2f})")
        elif operator == '<':
            descriptions.append(f"{field} < {value} (í˜„ì¬: {field_value:.2f})")
        elif operator == '>=':
            descriptions.append(f"{field} >= {value} (í˜„ì¬: {field_value:.2f})")
        elif operator == '<=':
            descriptions.append(f"{field} <= {value} (í˜„ì¬: {field_value:.2f})")
        elif operator == '==':
            descriptions.append(f"{field} == {value} (í˜„ì¬: {field_value})")
        elif operator == 'between':
            if isinstance(value, list) and len(value) == 2:
                descriptions.append(f"{value[0]} <= {field} <= {value[1]} (í˜„ì¬: {field_value:.2f})")

    return descriptions


# ========================================
# Task 3: ë§¤ìˆ˜ ì‹œê·¸ë„ ì €ì¥
# ========================================

def task_create_buy_signals(**context):
    """
    ë°œê²¬ëœ ì‹œê·¸ë„ì„ Supabase buy_signals í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤.

    Returns:
        ì €ì¥ëœ ì‹œê·¸ë„ ìˆ˜
    """
    logger.info("="*80)
    logger.info("Task 3: ë§¤ìˆ˜ ì‹œê·¸ë„ ì €ì¥ ì‹œì‘")
    logger.info("="*80)

    signals = context['task_instance'].xcom_pull(
        task_ids='run_screening',
        key='signals'
    )

    if not signals:
        logger.info("ì €ì¥í•  ì‹œê·¸ë„ì´ ì—†ìŠµë‹ˆë‹¤.")
        return 0

    supabase_engine = get_supabase_connection()
    saved_count = 0

    for signal in signals:
        try:
            # ì¤‘ë³µ ì²´í¬ (ì˜¤ëŠ˜ ê°™ì€ ì „ëµ + ì¢…ëª© ì‹œê·¸ë„ì´ ì´ë¯¸ ìˆëŠ”ì§€)
            check_query = """
                SELECT id FROM buy_signals
                WHERE strategy_id = :strategy_id
                AND symbol = :symbol
                AND DATE(created_at) = CURRENT_DATE
            """

            existing = pd.read_sql(
                check_query,
                supabase_engine,
                params={
                    'strategy_id': signal['strategy_id'],
                    'symbol': signal['symbol']
                }
            )

            if not existing.empty:
                logger.info(f"  ì¤‘ë³µ ì‹œê·¸ë„ ìŠ¤í‚µ: {signal['name']} ({signal['symbol']})")
                continue

            # ìƒˆ ì‹œê·¸ë„ ì €ì¥
            insert_query = """
                INSERT INTO buy_signals (
                    strategy_id, user_id, symbol, name,
                    price, change_percent, volume, indicators,
                    matched_conditions, is_read, notification_sent
                ) VALUES (
                    :strategy_id, :user_id, :symbol, :name,
                    :price, :change_percent, :volume, :indicators,
                    :matched_conditions, false, false
                )
            """

            with supabase_engine.begin() as conn:
                conn.execute(text(insert_query), {
                    'strategy_id': signal['strategy_id'],
                    'user_id': signal['user_id'],
                    'symbol': signal['symbol'],
                    'name': signal['name'],
                    'price': signal['price'],
                    'change_percent': signal['change_percent'],
                    'volume': signal['volume'],
                    'indicators': json.dumps(signal['indicators']),
                    'matched_conditions': signal['matched_conditions']
                })

            saved_count += 1
            logger.info(f"  âœ… ì‹œê·¸ë„ ì €ì¥: {signal['name']} ({signal['symbol']})")

        except Exception as e:
            logger.error(f"  âŒ ì‹œê·¸ë„ ì €ì¥ ì‹¤íŒ¨ ({signal.get('symbol')}): {str(e)}")

    logger.info(f"ì´ {saved_count}ê°œ ì‹œê·¸ë„ ì €ì¥ ì™„ë£Œ")

    # ì €ì¥ëœ ì‹œê·¸ë„ ID ëª©ë¡ ì „ë‹¬
    context['task_instance'].xcom_push(key='saved_count', value=saved_count)

    return saved_count


# ========================================
# Task 4: Slack ì•Œë¦¼ ì „ì†¡
# ========================================

def task_send_notifications(**context):
    """
    ë¯¸ë°œì†¡ ì‹œê·¸ë„ì„ Slackìœ¼ë¡œ ì•Œë¦¼ ì „ì†¡í•©ë‹ˆë‹¤.

    Returns:
        ì „ì†¡ëœ ì•Œë¦¼ ìˆ˜
    """
    logger.info("="*80)
    logger.info("Task 4: Slack ì•Œë¦¼ ì „ì†¡ ì‹œì‘")
    logger.info("="*80)

    saved_count = context['task_instance'].xcom_pull(
        task_ids='create_buy_signals',
        key='saved_count'
    )

    if saved_count == 0:
        logger.info("ì „ì†¡í•  ì•Œë¦¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return 0

    # ë¯¸ë°œì†¡ ì‹œê·¸ë„ ì¡°íšŒ
    supabase_engine = get_supabase_connection()

    query = """
        SELECT
            bs.id,
            bs.symbol,
            bs.name,
            bs.price,
            bs.change_percent,
            bs.volume,
            bs.indicators,
            bs.matched_conditions,
            ss.name as strategy_name
        FROM buy_signals bs
        JOIN saved_strategies ss ON bs.strategy_id = ss.id
        WHERE bs.notification_sent = false
        AND DATE(bs.created_at) = CURRENT_DATE
        ORDER BY bs.created_at DESC
    """

    pending_signals = pd.read_sql(query, supabase_engine)

    logger.info(f"ë¯¸ë°œì†¡ ì‹œê·¸ë„: {len(pending_signals)}ê°œ")

    sent_count = 0

    for _, signal in pending_signals.iterrows():
        try:
            # Slack ë©”ì‹œì§€ ìƒì„±
            message = format_slack_message(signal)

            # Slack ì „ì†¡ (ì‹¤ì œ Webhook URL ì„¤ì • í•„ìš”)
            send_slack_webhook(message)

            # ë°œì†¡ ì™„ë£Œ í‘œì‹œ
            update_query = """
                UPDATE buy_signals
                SET notification_sent = true, notified_at = NOW()
                WHERE id = :id
            """

            with supabase_engine.begin() as conn:
                conn.execute(text(update_query), {'id': signal['id']})

            sent_count += 1
            logger.info(f"  âœ… Slack ì•Œë¦¼ ì „ì†¡: {signal['name']} ({signal['symbol']})")

        except Exception as e:
            logger.error(f"  âŒ Slack ì•Œë¦¼ ì‹¤íŒ¨ ({signal['symbol']}): {str(e)}")

            # ì—ëŸ¬ ê¸°ë¡
            error_query = """
                UPDATE buy_signals
                SET notification_error = :error
                WHERE id = :id
            """

            try:
                with supabase_engine.begin() as conn:
                    conn.execute(text(error_query), {
                        'id': signal['id'],
                        'error': str(e)[:500]
                    })
            except:
                pass

    logger.info(f"ì´ {sent_count}ê°œ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")
    return sent_count


def format_slack_message(signal: pd.Series) -> str:
    """
    Slack ë©”ì‹œì§€ í¬ë§·íŒ…

    Args:
        signal: ì‹œê·¸ë„ ë°ì´í„°

    Returns:
        í¬ë§·ëœ ë©”ì‹œì§€
    """
    # ì§€í‘œ íŒŒì‹±
    indicators = signal.get('indicators')
    if isinstance(indicators, str):
        indicators = json.loads(indicators)

    # ì¡°ê±´ íŒŒì‹±
    conditions = signal.get('matched_conditions')
    if isinstance(conditions, str):
        conditions = json.loads(conditions)

    # ë©”ì‹œì§€ ìƒì„±
    message = f"""
ğŸ”” *ë§¤ìˆ˜ ì‹œê·¸ë„ ë°œìƒ!*

ğŸ“Š *ì „ëµ*: {signal['strategy_name']}
ğŸ“ˆ *ì¢…ëª©*: {signal['name']} ({signal['symbol']})
ğŸ’° *ê°€ê²©*: {signal['price']:,.0f}ì›
{'ğŸ“‰' if signal.get('change_percent', 0) < 0 else 'ğŸ“ˆ'} *ë“±ë½ë¥ *: {signal.get('change_percent', 0):.2f}%
ğŸ“Š *ê±°ë˜ëŸ‰*: {signal.get('volume', 0):,}

*ê¸°ìˆ ì  ì§€í‘œ*:
â€¢ RSI: {indicators.get('rsi', 'N/A')}
â€¢ MACD: {indicators.get('macd', 'N/A')}
â€¢ MA20: {indicators.get('ma_20', 'N/A'):,.0f}ì›

*ë§¤ì¹­ëœ ì¡°ê±´*:
"""

    for condition in conditions:
        message += f"â€¢ {condition}\n"

    message += f"\n_ì‹œê·¸ë„ ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}_"

    return message


def send_slack_webhook(message: str):
    """
    Slack Webhookìœ¼ë¡œ ë©”ì‹œì§€ ì „ì†¡

    Args:
        message: ì „ì†¡í•  ë©”ì‹œì§€

    Note:
        ì‹¤ì œ Webhook URLì„ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    import requests

    # TODO: ì‹¤ì œ Slack Webhook URLë¡œ ë³€ê²½
    SLACK_WEBHOOK_URL = os.getenv('SLACK_WEBHOOK_URL', '')

    if not SLACK_WEBHOOK_URL:
        logger.warning("Slack Webhook URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    payload = {
        "text": message,
        "username": "HappyStockLife Bot",
        "icon_emoji": ":chart_with_upwards_trend:"
    }

    response = requests.post(SLACK_WEBHOOK_URL, json=payload, timeout=10)
    response.raise_for_status()


# ========================================
# DAG Task ì •ì˜
# ========================================

fetch_task = PythonOperator(
    task_id='fetch_active_strategies',
    python_callable=task_fetch_active_strategies,
    provide_context=True,
    dag=dag
)

screening_task = PythonOperator(
    task_id='run_screening',
    python_callable=task_run_screening,
    provide_context=True,
    dag=dag
)

create_signals_task = PythonOperator(
    task_id='create_buy_signals',
    python_callable=task_create_buy_signals,
    provide_context=True,
    dag=dag
)

notify_task = PythonOperator(
    task_id='send_notifications',
    python_callable=task_send_notifications,
    provide_context=True,
    dag=dag
)

# ========================================
# Task ì˜ì¡´ì„± ì„¤ì •
# ========================================

fetch_task >> screening_task >> create_signals_task >> notify_task
